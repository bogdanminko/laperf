# =============================================================================
# La Perf Environment Configuration
# =============================================================================
# Copy this file to .env and adjust values as needed for your setup

# =============================================================================
# Backend Selection
# =============================================================================
# Options: AUTO (fallback), LM_STUDIO, OLLAMA, BOTH (comparison)
#
# AUTO - tries LM Studio first, falls back to Ollama (uses one)
# LM_STUDIO - forces LM Studio backend only
# OLLAMA - forces Ollama backend only
# BOTH - runs benchmarks on BOTH LM Studio AND Ollama for comparison
#
# Note: BOTH mode will run each benchmark twice (once per backend)
LLM_BACKEND=BOTH
VLM_BACKEND=BOTH

# API Keys (default works for local LM Studio/Ollama)
LLM_API_KEY=api-key
VLM_API_KEY=api-key

# Dataset sizes (number of samples to benchmark)
LLM_DATA_SIZE=10
VLM_DATA_SIZE=10

# =============================================================================
# Embedding Model Settings
# =============================================================================
EMBEDDING_MODEL_NAME=nomic-ai/modernbert-embed-base
EMBEDDING_BATCH_SIZE=32
EMBEDDING_DATA_SIZE=3000
EMBEDDING_MAX_LEN=1024

# =============================================================================
# LM Studio Settings
# =============================================================================
# Base URLs (default: local LM Studio on port 1234)
LMS_LLM_BASE_URL=http://127.0.0.1:1234/v1
LMS_VLM_BASE_URL=http://127.0.0.1:1234/v1

# Model names - MLX for Apple Silicon, GGUF for others
# The correct model will be auto-selected based on your device
LMS_MLX_LLM_MODEL_NAME=openai/gpt-oss-20b
LMS_GGUF_LLM_MODEL_NAME=openai/gpt-oss-20b
LMS_MLX_VLM_MODEL_NAME=qwen3-vl-8b-thinking-mlx
LMS_GGUF_VLM_MODEL_NAME=Qwen3-VL-8B-Thinking-GGUF

# =============================================================================
# Ollama Settings
# =============================================================================
# Base URLs (default: local Ollama on port 11434)
OLLAMA_LLM_BASE_URL=http://127.0.0.1:11434/v1
OLLAMA_VLM_BASE_URL=http://127.0.0.1:11434/v1

# Model names
OLLAMA_LLM_MODEL_NAME=gpt-oss:20b
OLLAMA_VLM_MODEL_NAME=qwen3-vl:8b

# =============================================================================
# Custom Provider Examples
# =============================================================================
# To use a different OpenAI-compatible provider (e.g., vLLM, TGI, LocalAI):
#
# 1. Set the backend to use only LM Studio or Ollama:
#    LLM_BACKEND=LM_STUDIO
#
# 2. Update the base URL to your provider:
#    LMS_LLM_BASE_URL=http://localhost:8000/v1
#
# 3. Set the model name your provider expects:
#    LMS_LLM_MODEL_NAME=Qwen/Qwen3-30B-Instruct
#
# 4. If your provider requires authentication, set the API key:
#    LLM_API_KEY=your-api-key-here
